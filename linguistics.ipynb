{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c7357983a8e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# text and words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# text and words\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.book import *\n",
    "\n",
    "# text consists of words\n",
    "# here are the first 100 of the Moby Dick text\n",
    "text1[:100]\n",
    "\n",
    "# and here is the start of the collection of inaugural addresses\n",
    "text4[:100]\n",
    "\n",
    "# by just using word counts, we can get an idea of the topic of a text.\n",
    "# for example like this\n",
    "# (Warning: this command requires matplotlib)\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
    "\n",
    "# or by hand:\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "# the list of inaugural addresses\n",
    "inaugural.fileids()\n",
    "\n",
    "# The first inaugural address\n",
    "inaugural.words(inaugural.fileids()[0])\n",
    "\n",
    "# relative frequency of \"freedom\"\n",
    "for address in inaugural.fileids():\n",
    "    address_words = inaugural.words(address)\n",
    "    percentage = address_words.count(\"freedom\") / len(address_words)\n",
    "    print(address, \"freedom:\", round(percentage, 3))\n",
    "\n",
    "# relative frequency of \"duties\"\n",
    "for address in inaugural.fileids():\n",
    "    address_words = inaugural.words(address)\n",
    "    percentage = address_words.count(\"duties\") / len(address_words)\n",
    "    print(address, \"duties:\", round(percentage, 3))\n",
    "\n",
    "# What are the most frequent words in the 1st inaugural address,\n",
    "# Washington 1789\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "address1 = inaugural.words(inaugural.fileids()[0])\n",
    "fd = FreqDist(address1)\n",
    "\n",
    "# 10 most frequent words\n",
    "fd.pprint(10)\n",
    "# or much more readably:\n",
    "fd.tabulate(10)\n",
    "# or like this:\n",
    "fd.most_common(10)\n",
    "\n",
    "# hm, that is not very informative. The problem is that we are seeing\n",
    "# a lot of stopwords.\n",
    "# Let's get rid of those\n",
    "from nltk.corpus import stopwords\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "for stopwords in en_stopwords: del fd[stopword]\n",
    "\n",
    "# now let's try again\n",
    "fd.most_common(20)\n",
    "# better, though we haven't caught everything.\n",
    "\n",
    "# compare that to the most recent inaugural address:\n",
    "# Obama 2009\n",
    "addressO = inaugural.words(inaugural.fileids()[-1])\n",
    "fdO = FreqDist(addressO)\n",
    "for stopword in en_stopwords: del fdO[stopword]\n",
    "fdO.most_common(20)\n",
    "\n",
    "# or we can look at positive and negative sentiment\n",
    "from nltk.corpus import opinion_lexicon\n",
    "# we have two wordlists, one positive and one negative\n",
    "opinion_lexicon.fileids()\n",
    "# here is what is in there:\n",
    "opinion_lexicon.words(\"negative-words.txt\")\n",
    "opinion_lexicon.words(\"positive-words.txt\")\n",
    "\n",
    "# What fraction of each inaugural address\n",
    "# consists of positive or negative words?\n",
    "poswords = set(opinion_lexicon.words(\"positive-words.txt\"))\n",
    "negwords = set(opinion_lexicon.words(\"negative-words.txt\"))\n",
    "\n",
    "for address in inaugural.fileids():\n",
    "    address_words = inaugural.words(address)\n",
    "    percpos = len([w for w in address_words if w in poswords  ]) / len(address_words)\n",
    "    percneg = len([w for w in address_words if w in negwords  ]) / len(address_words)\n",
    "    print(address, \"pos:\", round(percpos, 3), \"neg:\", round(percneg, 3))\n",
    "\n",
    "# Looks like everybody uses more positive than negative opinion words.\n",
    "\n",
    "##########\n",
    "# But a text is more than a \"bag of words\"! What else can we do?\n",
    "\n",
    "# The simplest next step that we can take is to take\n",
    "# word sequence into account\n",
    "# This is based on counting word sequences\n",
    "for ngram in list(nltk.ngrams(list(text4), 3))[:10]: print(ngram)\n",
    "\n",
    "# some magic that determines the probability of one word\n",
    "# following another\n",
    "ns = list(nltk.ngrams(text4, 2))\n",
    "cpd = nltk.ConditionalProbDist(nltk.ConditionalFreqDist(ns), nltk.MLEProbDist)\n",
    "\n",
    "# what words do we know about?\n",
    "cpd.conditions()\n",
    "\n",
    "# for example, what can follow \"selfish\"?\n",
    "cpd[\"selfish\"].samples()\n",
    "\n",
    "# if we've just seen \"selfish\", what's the most likely next word?\n",
    "cpd[\"selfish\"].max()\n",
    "\n",
    "# and how likely is it that the next word is going to be \"men\"?\n",
    "cpd[\"selfish\"].prob(\"men\")\n",
    "\n",
    "# So, what do you think this is good for?\n",
    "\n",
    "# Let's generate some text at random\n",
    "word = \"I\"\n",
    "for i in range(100):\n",
    "    print(word, end = \" \")\n",
    "    word = cpd[ word].generate()\n",
    "\n",
    "print(word)\n",
    "\n",
    "# And what could this be good for?\n",
    "\n",
    "###############\n",
    "# Beyond raw text: linguistic analysis\n",
    "\n",
    "# A bit of text from the Washington Post, January 15, 2015\n",
    "\n",
    "# <<< at this point, grab some text from a current news article and store it \n",
    "# in the variable 'text', like so: >>>\n",
    "\n",
    "text = \"\"\" This is a placeholder.\n",
    "Please place some more interesting text here.\"\"\"\n",
    "\n",
    "# Splitting the text into words\n",
    "text.split()\n",
    "\n",
    "# Or like this: do you see the difference?\n",
    "nltk.word_tokenize(text)\n",
    "\n",
    "# Part of speech: is this a noun, an adjective, a verb?\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "nltk.pos_tag(words)\n",
    "\n",
    "####\n",
    "# syntactic analysis\n",
    "# Here is an example of what we want the system to do\n",
    "from nltk.corpus import treebank\n",
    "print(treebank.parsed_sents('wsj_0001.mrg')[0])\n",
    "\n",
    "# or like this\n",
    "treebank.parsed_sents('wsj_0001.mrg')[0].draw()\n",
    "\n",
    "###########\n",
    "# Can we learn about semantic similarity from data?\n",
    "text4.similar(\"freedom\")\n",
    "text2.similar(\"freedom\")\n",
    "\n",
    "# Here's how this works:\n",
    "text4.common_contexts([\"freedom\", \"peace\"])\n",
    "text2.common_contexts([\"freedom\", \"hair\"])\n",
    "# (This is a pretty simplistic approach.\n",
    "# You can do the same thing in more sophisticated ways,\n",
    "# and get better answers. In particular you should use more data\n",
    "# than a single book. Yes, a book counts as \"too little data\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
